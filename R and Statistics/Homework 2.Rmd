---
title: "Homework 2: Data wrangling and model evaluation"
output:
  rmarkdown::html_document:
    theme: lumen
    toc: true
    toc_float: true
date: "2023-06-21"
---
```{r warning = FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(tidymodels)
library(mosaicData)
library(lubridate)
```
8.2 Data Wrangling Practice

The number of daily births in the US varies over the year and from day to day. What’s surprising to many people is that the variation from one day to the next can be huge. In this section we’ll use basic data wrangling skills to understand some of these dynamics. The Birthdays data in the mosaicData package gives the number of births recorded on each day of the year in each state from 1969 to 1988.
```{r}
data("Birthdays")
head(Birthdays, 3)
```

2. Minnesota
Create a data set with only births in Minnesota (MN) in 1980, and sort the days from those with the most births to those with the fewest. Show the first 6 rows.
```{r}
#Births only in MN in 1980
MN_Birthdays <- Birthdays %>% 
  filter(year==1980,state=="MN")
head(MN_Birthdays)

#sort the days from those with the most births to those with the fewest.
MN_Birthdays %>% 
  arrange(desc(births)) %>% 
  head()
```
```{r}
#3. Averaging births
#Use dplyr verbs to calculate the following.
#The average number of daily births. Your answer should be 1 number.
Birthdays %>% 
  summarize(mean(births))

#The average number of daily births in each year. Your answer should be 20 numbers, 1 per year.
Birthdays %>% 
  group_by(year) %>% 
  summarize(mean(births))

#The average number of daily births in each year, by state. Your answer should be 1020 numbers, 1 per year and state combination (there are 51 states, including D.C.).
Birthdays %>% 
  group_by(year,state) %>% 
  summarize(mean(births)) %>% 
  head()

```
```{r}
#4. Working with dates
#Create a new data set, daily_births, that adds up the total number of births in the U.S. on each day (across all the states). Your table should have 7305 rows and 2 columns.
daily_births <- Birthdays %>% 
  group_by(date) %>% 
  summarize(total = sum(births)) %>% 
  mutate(year = year(date), 
         month = month(date,label=TRUE),
         mday = mday(date),
         weekday = wday(date,label=TRUE))

daily_births
```
```{r}
#5. Seasonality
#Plot total births (y axis) vs date (x axis). Comment.
ggplot(daily_births,aes(x=date,y=total))+
  geom_point()

#Digging deeper into the observed seasonality, construct 2 separate visualizations of: births by day of the week and births by month.
ggplot(daily_births,aes(x=weekday,y=total))+
  geom_boxplot()
ggplot(daily_births,aes(y=total,x=month))+
  geom_boxplot()

#Summarize your observations. When are the most babies born? The fewest?
#The most babies are born during September and on Tuesdays. The least are born during January and on Sunday.
```
```{r}
#6. Sleuthing
#In your plot of daily births vs date in the previous exercise (part a), one goofy thing stands out: there seem to be 2-3 distinct groups of points.
#To zoom in on the pattern, create a new subset of daily_births which only includes the 2 year span from 1980-81.
births_80_81 <- daily_births %>%
  filter(year %in% c(1980,1981))

#Using the 1980-81 data, plot daily births vs date and add a layer that explains the distinction between the distinct groups.
ggplot(births_80_81,aes(x=date,y=total,color=weekday))+
  geom_point()

#There are some exceptions to the rule in part b, ie. some cases that should belong to group 1 but behave like the cases in group 2. Explain why these cases are exceptions - what explains the anomalies / why these are special cases?
#They're holidays

```

```{r}
#7. Superstition
#Some people are superstitious about Friday the 13th.
#Create a new data set, friday_only, that only contains births that occur on Fridays. Further, create a new variable within this data set, fri_13, that indicates whether the case falls on a Friday in the 13th date of the month.
friday_only <- daily_births %>% 
  filter(weekday=="Fri") %>% 
  mutate(fri_13=(mday==13))

#Using the friday_only data, construct and interpret a visualization that illustrates the distribution of births among Fridays that fall on & off the 13th. Do you see any evidence of superstition of Friday the 13th births? (See this article for more discussion.)
ggplot(friday_only,aes(x=total,fill=fri_13))+
  geom_density(alpha=0.5)

```
There are less births on Friday 13th.

8.3 Model Building
In the remainder of the homework, we’ll return to modeling. Continuing with our humans analysis, let Y be body fat percentage (body_fat) and (X1,X2,...,Xp) be a set of p possible predictors of Y.
```{r}
# Import & wrangle the data 
humans <- read.csv("https://raw.githubusercontent.com/ajohns24/data/main/bodyfatsub.csv") %>%
  dplyr::select(-c(Density, HeightFt)) %>% 
  rename(body_fat = BodyFat) %>% 
  rename_all(tolower)
head(humans)
```
In the examples we’ve seen thus far, the appropriate model has been clear. For example, one of our goals today was to understand the relationship between body_fat and chest circumference (X1), thus our model had the form body_fat ~ chest:

Y=β0+β1X1+ε

In other scenarios, the appropriate model is less clear. Consider a new goal: using any or all of the p

predictors available to us, build the “best” model of body_fat. Using all 11 predictors might be a bad idea - it might both be unnecessarily complicated or cause us to overfit our model to our sample data. In general, in building the “best” model, we’re often balancing 2 sometimes conflicting criteria:

    prediction accuracy
    we want a model that produces accurate predictions and generalizes well to the broader population

    simplicity / parsimony
    depending upon the context, we might also want a model that uses as few predictors as possible. This
        will be easier to interpret;
        eliminates unnecessary noise & multicollinearity (redundancies among the predictors);
        cuts costs by not requiring the continued measurement and storage of unnecessary predictors.


With these criteria in mind, there are three broad model selection techniques. We’ll only consider the first of these.

    Shrinkage / regularization
        Goal: Fit a model with all xi

    , but shrink / regularize their coefficients toward or to 0 to create sparse models.
    Methods: LASSO, ridge regression, elastic net

Subset selection

    Goal: Identify a subset of predictors xi

to include in our model of y

    .
    Methods: best subset selection, backward stepwise selection, forward stepwise selection
    Drawbacks: These methods are intuitive, but give overly optimistic conclusions about the significance of the remaining predictors. They’re rarely applied in big data settings.
    Learning more: If you’d like to learn more, check out this video.

Dimension reduction

    Goal: Reduce the number of predictors by creating a new set of predictors that are linear combinations of the xi

. Then use this new set of predictors to model y
.
Methods: Principal components regression
Drawbacks: Though these methods can produce accurate predictions, the predictors (thus our models) lose some contextual meaning.
Learning more: You’ll likely address this topic in your machine learning module. To learn about the principal components approach to reducing the dimensions of our predictors, check out this video.

8. Starting with linear regression
For a point of comparison, let’s first consider the linear regression model of body_fat by all 11 predictors in humans:
```{r}
# Specify our modeling method (the same as usual)
lm_spec <- linear_reg() %>%
  set_mode("regression") %>% 
  set_engine("lm")

# Estimate the model
model_lm <- lm_spec %>% 
  fit(body_fat ~ ., data = humans)

#Using set.seed(253), use 10-fold cross-validation to estimate the MAE of this model.
#Specify the workflow
model_lm_workflow <- workflow() %>% 
  add_formula(body_fat~.) %>% 
  add_model(lm_spec)

#Build and evaluate the model on 10 sets
set.seed(253)
model_lm_cv <- fit_resamples(
  model_lm_workflow,
  resamples = vfold_cv(humans, v = 10), 
  metrics = metric_set(mae)
)
model_lm_cv %>% 
  collect_metrics()
```
So the 10-fold cross-validation (CV) estimate of the MAE is 3.685.

"Python for Machine Learning" book is good for doing exercises

Shrinkage methods for model building include LASSO, ridge regression, and elastic net. These are all variations on the same theme. Thus here we’ll focus on the LASSO. You’re strongly encouraged to watch this LASSO video before moving on. A written summary is given below but likely doesn’t suffice on its own.

    LASSO: least absolute shrinkage and selection operator

    Our goal is to build a regression model

    y=^β0+^β1x1+⋯+^βpxp+ε

We can summarize the residuals of each case i

and the cumulative “residual sum of squares” (ie. sum of the squared residuals) by:

yi−^yi=yi−(^β0+^β1xi1+⋯+^βpxip)RSS=n∑i=1(yi−^yi)2

Criterion

Identify the model coefficients ^β1,^β2,...^βp that minimize the penalized residual sum of squares:

  RSS+λp∑j=1|^βj|

Properties

As λ increases, coefficients shrink toward 0. This is called shrinkage or regularization.

Picking tuning parameter λ is a goldilocks problem, we don’t want it to be too big or too small.

When λ=0 (too small), LASSO is equivalent to least squares, thus doesn’t simplify the model.

When λ is too big it also kicks out useful predictors!

When λ is just right, LASSO helps build parsimonious or sparse models which only include the most relevant predictors. These are less prone to overfitting.

We can use cross-validation to help choose an appropriate λ.

10.Lasso Code
Let’s apply the LASSO algorithm to our humans analysis. Our goal here is to improve the interpretability and predictive accuracy of body_fat by using only a subset of our 11 predictors. To this end, we’ll use the tidymodels package. The required code is much longer than we’re used to. The good news is that this same code can be adapted to a wide range of machine learning algorithms. And remember don’t worry about memorizing the code – focus on the overall structure.
```{r}
# STEP 1: Specify our LASSO modeling method
lasso_spec <- 
  linear_reg(penalty = tune(), mixture = 1) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet") 

# STEP 2: Specify our modeling workflow 
# We'll use this for each possible lambda penalty parameter & training set
lasso_workflow <- workflow() %>%
  add_formula(body_fat ~ .) %>%
  add_model(lasso_spec)

# STEP 3: Build the LASSO model using a range of tuning parameters
# Evaluate each model using 10-fold CV MAE

# Set the seed to get reproducible results
set.seed(2000)

# Apply the model_1_workflow to build the model...
#   using each of the 50 possible lambda penalty parameters in the "grid".
# Then calculate the 10-fold CV MAE for each LASSO model.
lasso_grid <- tune_grid(
  lasso_workflow,
  grid = grid_regular(penalty(), levels = 50),
  resamples = vfold_cv(humans, v = 10),
  metrics = metric_set(mae)
)
```

a) How does the code in STEP 1, the model specification, compare for LASSO models (lasso_spec) vs ordinary linear regression models (lm_spec)?

For linear regression models: 
lm_spec <- linear_reg() %>%
  set_mode("regression") %>% 
  set_engine("lm")
They're similar, but lasso uses penalty and mixture.

b) In STEP 3, we fit the LASSO model for each possible λ value in a grid or range. Check out the possible λ values: grid_regular(penalty(), levels = 50). What’s the smallest tuning parameter we’re considering? The largest?

Our smallest tuning parameter is 1.0e-10, and the largest is 1.0.

```{r}
grid_regular(penalty(),levels=50)
```


c) To build any model, we need data. Where in this code do we specify that we want to build the LASSO using the data from the humans dataset?

We specify it in the resamples whilst defining lasso_grid.

11. Tuning & picking λ
Our goal is to pick which of our 50 possible λ values, hence which LASSO model, is “best”. To this end, for each possible λ penalty value, we can examine & compare the corresponding 10-fold CV MAE (mean):

```{r}
lasso_grid %>% 
  collect_metrics()
```
To help make sense of these numbers, let’s plot of the 10-fold CV MAE (y-axis) for the LASSO model using each of the 50 λ values (x-axis):

```{r}
# Plot metrics for each LASSO model
# scale_x_continuous plot the lambda values on the original, not log, scale
autoplot(lasso_grid) + 
  scale_x_continuous()

# Now on the log10 scale
autoplot(lasso_grid) + 
  scale_x_log10()
```

a) Use this plot to describe the “goldilocks” problem of selecting λ: What are the consequences when λ is too small? Too large?

If lambda is too small, the mae goes higher. But also if lambda is too large, the mae gets very large.

b) Based on the plot alone, roughly which value of the λ
tuning parameter produces a LASSO model with the smallest CV MAE? Check your approximation: I think 0.01
```{r}
# Identify the lambda penalty which produced the lowest CV MAE
best_penalty <- select_best(lasso_grid, metric = "mae")
best_penalty
```

c) The LASSO model with the smallest CV MAE isn’t necessarily the best. Instead, we might prefer a simpler model for which the CV MAE is larger but still within one standard error of the minimum CV MAE, hence roughly as good. Identify the λ that produces this “best” model and report the corresponding CV MAE. Connect these numerical results with the plots above. 0.01? Ok, notice where 0.39 is on the graph.
```{r}
# Identify which lambda tuning parameter is "best" 
best_penalty_1_se <- select_by_one_std_err(lasso_grid, metric = "mae", desc(penalty))
best_penalty_1_se
```

12. Examining the best LASSO model
Now that you’ve identified the “best” λ, let’s finalize our LASSO model using that λ value:
```{r}
# Finalize the LASSO model
final_lasso <- lasso_workflow %>% 
  finalize_workflow(parameters = best_penalty_1_se) %>% 
  fit(data = humans)

# Check out the final model
final_lasso %>% 
  tidy()
```

a) The estimate column in the tidy() summary reports the predictor coefficients in the final_lasso model. Which predictors are kept in this model, i.e. have non-0 coefficients? Do any of these surprise you?

Height, abdomen, age, and wrist are kept in this model. I'm surprised wrist is kept.

b) If you had to choose between your best LASSO model and the above linear regression model with all 13 predictors and a CV MAE of roughly 3.6, which would you choose?

Our best lasso model has a lambda of 3.9, which is close to the CV MAE of the linear regression model with 13 predictors. I'd choose the lasso since it has less predictors and a similar MAE.

13. Visualizing LASSO shrinkage across possible λ
In the previous exercise you examined the LASSO model coefficients under just the “best” of the 50 λ penalty parameters we considered. Let’s use a visualization to compare these results to those of the other 49 LASSO models we considered:
```{r}
# Get output for each LASSO
all_lassos <- final_lasso %>% 
  extract_fit_parsnip() %>%
  pluck("fit")

# Plot coefficient paths as a function of lambda
plot(all_lassos, xvar = "lambda", label = TRUE, col = rainbow(20))

# Codebook for which variables the numbers correspond to
rownames(all_lassos$beta)

# e.g., What are variables 2 and 4?
rownames(all_lassos$beta)[c(2,4)]
```
There’s a lot of information in this plot!

Each colored line corresponds to a different predictor. The small number to the left of each line indicates the predictor by its order in the rownames() list.
The x-axis reflects the range of different λ values we considered, reported on the log scale.
At each λ, the y-axis reflects the predictor coefficients in the corresponding LASSO model.
At each λ, the numbers at the top of the plot indicate how many predictors remain in the corresponding model.
We’ll process this information in the 3 following exercises.

14. plot: examining coefficients at a specific λ
Let’s narrow in on specific λ values (on the x-axis).

a) Check out the coefficients at the smallest λ, hence smallest log(λ), value on the far left. Confirm that these appear to be roughly equivalent to the least squares coefficient estimates.

The coefficients do appear to be roughly correct by using tidy().
```{r}
model_lm %>% 
  tidy()
```

b) Check out the coefficients at λ=0.391
(log(λ)≈−1). Confirm that these are roughly equivalent to the coefficients of your final_lasso.

The coefficients do look roughly correct.

15.plot: examining specific predictors
Let’s narrow in on specific model predictors (lines).

a) What predictor corresponds to the line labeled with the number 3?

It is height.
```{r}
head(final_lasso)
```

b) Approximate predictor 3’s coefficient for the LASSO model which uses λ≈0.37, i.e. log(λ)≈−1.

About -0.25.

c) At what log(λ) does predictor 3’s coefficient start to shrink?

About -0.5.

d) At what log(λ) does predictor 3’s coefficient effectively shrink to 0, hence the predictor get dropped from the model?

About 0.25.

16. plot: big picture
a) What aspect of this plot reveals / confirms the LASSO shrinkage phenomenon?

We see multiple predictors shrink to 0 and get dropped from the model.

b) Which of the 13 original predictors is the most “important” or “persistent” variable, ie. sticks around the longest?

6, abdomen

c) Which predictor is one of the least persistent?

9, knee

d) How many predictors would remain in the model if we used the λ which produced the minimum CV MAE, λ≈0.0418 (log(λ)=−3.2)?

It's still 4 predictors by looking at the top of the graph.

8.4 Bias-variance trade-off
In today’s discussion & homework, we’ve seen that there’s a goldilocks problem in model building: if we use too few predictors, we lose some explanatory power; if we use too many, we risk overfitting the model to our own sample data. This conundrum is related to the bias-variance trade-off:

    Bias
    Across different possible sets of sample data, how far off do the sample models’ depictions of y tend to be?

    Variance
    How stable is the model across different samples?

Ideally, both bias and variance would be low. BUT when we improve one of the features, we hurt the other. For example, consider two possible models of body_fat: body_fat ~ 1 (a model with only an intercept term & no predictors) and body_fat ~ poly(chest, 7). Check out the behavior of the simpler body_fat ~ 1 model across 4 different samples of 40 adults each:

These 2 models illustrate the extremes:

    The simpler model (body_fat ~ 1) has high bias (it doesn’t explain much about body_fat) but low variability (it doesn’t vary much from sample to sample). Thus this model provides stability, but at the cost of not much info about body_fat. It is too rigid.
    
    The complicated model (body_fat ~ poly(chest, 7)) has low bias (each of the models captures more detail in body_fat) but high variability (it varies a lot from sample to sample). Thus this model provides more detailed info about body_fat, but at the cost of not being broadly generalizable beyond the sample at hand. It is too flexible / wiggly.
    
The goal in model building is to be in between these two extremes!

17. LASSO and the bias-variance trade-off
Think back to the LASSO model above which depended upon tuning parameter λ.
a) For which values of λ (small or large) will LASSO be the most biased?

b) For which values of λ (small or large) will LASSO be the most variable?

c) The bias-variance trade-off also comes into play when comparing across methods. Consider LASSO vs least squares:

    Which will tend to be more biased?
    Which will tend to be more variable?
    When will LASSO beat least squares in the bias-variance trade-off game?

8.5 Solutions

10. Lasso code
a) We add penalty = tune(), mixture = 1 to the linear_reg() function. NOTE: mixture = 1 specifies LASSO regression and penalty = tune() asks R to pick a range of tuning parameters λ for us. We could also supply our own. Further, we use the glm instead of the lm engine to build the model.

b) The λ values range from roughly 0 to exactly 1.

c) In step 3, we indicated that we wanted to tune the LASSO using resampling / cross-validation from the humans dataset.

11. Tuning
a) When λ is too small, the model contains too many predictors thus is overfit to the sample data, thus has high CV MAE. When lambda is too big, the model contains too few predictors, thus doesn’t contain useful info about body_fat, thus CV MAE is high. This will be easier to understand after exercise 13+.

b) 0.0373ish

c) Lasso using λ≈0.391 has a CV MAE of roughly 3.68.

12. Examining the best LASSO
a) 4 predictors: age, height, abdomen, wrist

b) Given that its CV MAE (roughly 3.68) is very similar to that of the least squares model (roughly 3.6) while only using 4 predictors (instead of all 13), I’d choose the LASSO. It’s much simpler.

14. plot: examining coefficients
a) Yep
b) Yep

15. plot: examining specific predictors
a) height
b) roughly -0.25
c) roughly -0.5
d) roughly 0.25

16. plot: big picture
a) The predictor coef lines shrink to 0 as λ increases.
b) abdomen (variable 6)
c) weight (2), knee (9), and others
d) 4

17. LASSO and the bias-variance trade-off
a) large. As λ increases, the model has fewer predictors and will be more rigid.
b) small. When λ is small, the model will include all / most predictors, thus might be overfit to our sample (and vary a lot from sample to sample).
c)

    LASSO (it tends to be more rigid)
    LM
    When using all predictors causes overfitting (generally when we have a lot of predictors relative to a smaller sample size)

6. The fivethirtyeright package has more recent data

7. See Wickham and Grolemund, Date and Times with lubridate, for more detail.